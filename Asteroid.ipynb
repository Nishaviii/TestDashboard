{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Asteroid.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nishaviii/TestDashboard/blob/master/Asteroid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6djebJ8Jxpu4"
      },
      "source": [
        "import torch\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJSHecDixpu_"
      },
      "source": [
        "class AsteroidDataset(Dataset):\n",
        "    def __init__(self, csv_path, transform=None):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        imputer = SimpleImputer(strategy=\"median\")\n",
        "        ordinal_encoder = OrdinalEncoder()\n",
        "        \n",
        "        NEA_data= self.data.drop(\"name\", axis=1)\n",
        "        \n",
        "        num_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),('std_scaler', StandardScaler())])\n",
        "        NEA_data_num = NEA_data.drop(\"pha\", axis=1)\n",
        "        num_attribs = list(NEA_data_num)\n",
        "        cat_attribs = [\"pha\"]\n",
        "        \n",
        "        full_pipeline = ColumnTransformer([(\"num\", num_pipeline, num_attribs),(\"cat\", OrdinalEncoder(), cat_attribs)])\n",
        "        \n",
        "        NEA_data = NEA_data.dropna(subset=[\"pha\"])\n",
        "        self.data = full_pipeline.fit_transform(NEA_data)\n",
        "        self.data = torch.from_numpy(self.data)\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # This method should return only 1 sample\n",
        "        return self.data[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def _getsplit_(self, batch_size=16,test_split=.3,validation_split=1/3,shuffle_dataset=True,random_seed=42):\n",
        "        \n",
        "        # Creating data indices for training and test splits:\n",
        "        dataset_size = len(self.data)\n",
        "        #print(dataset_size)\n",
        "        indices = list(range(dataset_size))\n",
        "        split = int(np.floor(test_split * dataset_size))\n",
        "        if shuffle_dataset :\n",
        "            np.random.seed(random_seed)\n",
        "            np.random.shuffle(indices)\n",
        "        train_indices, test_indices = indices[split:], indices[:split]\n",
        "        \n",
        "        # Creating data indices for testing and validation splits:\n",
        "        dataset_size = len(test_indices)\n",
        "        #print(dataset_size)\n",
        "        indices = list(range(dataset_size))\n",
        "        split = int(np.floor(validation_split * dataset_size))\n",
        "        if shuffle_dataset :\n",
        "            np.random.seed(random_seed)\n",
        "            np.random.shuffle(indices)\n",
        "        test_indices, validation_indices = indices[split:], indices[:split]\n",
        "\n",
        "        # [:split] - Slice elements from the beginning to index split(not included):\n",
        "        # [split:] - Slice elements from index split to the end of the array: \n",
        "        \n",
        "        # Creating PT data samplers and loaders:\n",
        "        train_sampler = SubsetRandomSampler(train_indices)\n",
        "        test_sampler = SubsetRandomSampler(test_indices)\n",
        "        validation_sampler = SubsetRandomSampler(validation_indices)\n",
        "\n",
        "        train_loader = torch.utils.data.DataLoader(self.data, batch_size=len(train_indices), \n",
        "                                           sampler=train_sampler)\n",
        "        test_loader = torch.utils.data.DataLoader(self.data, batch_size=len(test_indices), \n",
        "                                           sampler=test_sampler)\n",
        "        validation_loader = torch.utils.data.DataLoader(self.data, batch_size=len(validation_indices),\n",
        "                                                sampler=validation_sampler)\n",
        "\n",
        "        return train_loader,test_loader,validation_loader\n",
        "    \n",
        "    def load_data(self): #load a single batch of data\n",
        "        train_loader,test_loader,validation_loader =self._getsplit_()\n",
        "        for data in train_loader:\n",
        "            train = data\n",
        "        for data in test_loader:\n",
        "            test = data\n",
        "        for data in validation_loader:\n",
        "            validation = data\n",
        "        return train,test,validation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL-9eaBUxpvB",
        "outputId": "5e9bd4a8-fad7-45c6-da9a-635c9c1e67ec"
      },
      "source": [
        "dataset = AsteroidDataset(r\"C:\\Users\\Nishavi Ranaweera\\results (3).csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3185: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  if (yield from self.run_code(code, result)):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ko3wSxpxpvC",
        "outputId": "79a88e94-c237-4d79-f44b-c5a1ffa0e68a"
      },
      "source": [
        "dataset.__getitem__(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.8431, -0.0028,  0.2310, -0.8579, -1.0385,  0.5461,  0.0000],\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tENOPQNZxpvC"
      },
      "source": [
        "train,test,validation = dataset.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T_UCaIUxpvD",
        "outputId": "7a1da098-127e-48ee-d021-9425f8db7813"
      },
      "source": [
        "validation.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([103409, 7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ULoyL40xpvD"
      },
      "source": [
        "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
        "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)\n",
        "validationset = torch.utils.data.DataLoader(validation, batch_size=10, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXrbeAO6xpvE",
        "outputId": "ede69896-039c-4be3-b808-739763f9fdc1"
      },
      "source": [
        "pha_train = []\n",
        "for index, data in trainset: \n",
        "    if(data[6]):\n",
        "        pha_data "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-10-6c3c0bb6a393>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpha_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mpha_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP_XYqKrxpvE"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6G3U23fxpvE",
        "outputId": "03158cc8-4a8b-45cd-a3c3-5fd3917b1fc2"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(1*6, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 64)\n",
        "        self.fc4 = nn.Linear(64, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "mlp = MLP()\n",
        "print(mlp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (fc1): Linear(in_features=6, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (fc4): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNx2aB-dxpvF",
        "outputId": "ee2bcb0f-996a-47b7-dc8e-2571ef48b26f"
      },
      "source": [
        "X = torch.randn((1,6))\n",
        "output = mlp(X)\n",
        "output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.6630, -0.7243]], grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I89OAlFfxpvF"
      },
      "source": [
        "#calculate loss and specify our optimizer\n",
        "import torch.optim as optim\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()  #loss_function is what calculates \"how far off\" our classifications are from reality\n",
        "optimizer = optim.Adam(mlp.parameters(), lr=0.001) #optimizer adjusts our model's adjustable parameters like the weights, to slowly, over time, fit our data. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "effegalyxpvG",
        "outputId": "4581e95f-577b-4a99-8446-e1728c472788"
      },
      "source": [
        "for epoch in range(3): # 3 full passes over the data\n",
        "    for data in trainset:  # `data` is a batch of data\n",
        "        X = data[:, :6].float()\n",
        "        y = data[:,6].long()\n",
        "       # X,y = data # X is the batch of features, y is the batch of targets.\n",
        "        mlp.zero_grad()  # sets gradients to 0 before loss calc. You will do this likely every step.\n",
        "        output = mlp(X)  # pass in the reshaped batch (recall they are 28x28 atm)\n",
        "        loss = F.nll_loss(output, y)  # calc and grab the loss value\n",
        "        loss.backward()  # apply this loss backwards thru the network's parameters\n",
        "        optimizer.step()  # attempt to optimize weights to account for loss/gradients\n",
        "    print(loss)  # print loss. We hope loss (a measure of wrong-ness) declines! "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0010, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0006, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I76bQ4mxpvG",
        "outputId": "b852d9cd-5352-4d7f-bdaf-8252b3a4bbd7"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for data in testset:\n",
        "        X = data[:, :6].float()\n",
        "        y = data[:,6].long()\n",
        "        output = mlp(X)\n",
        "        #print(output)\n",
        "        for idx, i in enumerate(output):\n",
        "            y_pred.append(torch.argmax(i))\n",
        "            if torch.argmax(i) == y[idx]:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "print(\"Accuracy: \", round(correct/total, 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD8seVaSxpvH"
      },
      "source": [
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for data in trainset:\n",
        "        X = data[:, :6].float()\n",
        "        y = data[:,6].long()\n",
        "        output = mlp(X)\n",
        "        #print(output)\n",
        "        for idx, i in enumerate(output):\n",
        "            y_pred.append(torch.argmax(i))\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxFFofVQxpvH",
        "outputId": "26a53a79-79f6-48d0-eb6f-6da0d13b798c"
      },
      "source": [
        "len(y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "382254"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUpkD37dxpvH",
        "outputId": "eb5e4e6f-6161-44ba-e747-8ec8d4530ede"
      },
      "source": [
        "train[:,6].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([382254])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpfRzmAkxpvH"
      },
      "source": [
        "99% Accuracy cannot be considered as the preferred performance measure as the datset is skewed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31qBZ_owxpvI"
      },
      "source": [
        "Confusion matrix -The general idea is to count the number of times instances of class A are classified as class B."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwMT3-NHxpvI",
        "outputId": "bb4d9b1c-16da-4aa1-b60f-2938778a2454"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(train[:,6], y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[381727,      0],\n",
              "       [   527,      0]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-402db0xpvI"
      },
      "source": [
        "row- actual class \n",
        "cloumn - predicted class\n",
        "381727 (true negatives) were correctly classified as non-phas , 0 (false positives) were wrongly classifed as phas\n",
        "527 (flase negatives) were wrongly classified as non-phas , 0 (true postives) were correctly classified as phas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKS_XJC1xpvI"
      },
      "source": [
        "We will write them as callable classes instead of simple functions so that parameters of the transform need not be \n",
        "passed everytime it’s called. \n",
        "For this, we just need to implement __call__ method and if required, __init__ method. \n",
        "We can then use a transform like this:https://pytorch.org/tutorials/beginner/data_loading_tutorial.html - Transforms write seperate classes for each pre-processing method\n",
        "https://stackoverflow.com/questions/55588201/pytorch-transforms-on-tensordataset/55593757"
      ]
    }
  ]
}