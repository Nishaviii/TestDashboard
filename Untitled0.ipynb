{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPOryh2yef2DoHVMXXji39A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nishaviii/TestDashboard/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEOxdpqnIkgy"
      },
      "source": [
        "import torch\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os, shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from torchvision import datasets\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from Asteroid import AsteroidDataset\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKi2HGu9JBCn"
      },
      "source": [
        "uploaded = files.upload()\n",
        "dataset = AsteroidDataset('dataset.csv')\n",
        "train_loader,test_loader = dataset.load_data()\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(1*6, 16)\n",
        "        self.fc2 = nn.Linear(16, 16)\n",
        "        self.fc3 = nn.Linear(16, 16)\n",
        "        self.fc4 = nn.Linear(16, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "mlp = MLP()\n",
        "print(mlp)\n",
        "#loss_function calculates \"how far off\" our classifications are from reality\n",
        "loss_function = nn.CrossEntropyLoss()  \n",
        "#optimizer adjusts our model's adjustable parameters \n",
        "#like the weights, to slowly, over time, fit our data.\n",
        "optimizer = optim.Adam(mlp.parameters(), lr=0.002) \n",
        "\n",
        "epoch_loss = []\n",
        "for epoch in range(50): \n",
        "  for data in train_loader:  \n",
        "      X = data[:, :6].float()\n",
        "      y = data[:,6].long()\n",
        "      # sets gradients to 0 before loss calculation\n",
        "      mlp.zero_grad()   \n",
        "      output = mlp(X) \n",
        "      # calculate loss \n",
        "      loss = loss_function(output, y)\n",
        "      # backpropagate the loss\n",
        "      loss.backward() \n",
        "      # attempt to optimize weights to account for loss/gradients\n",
        "      optimizer.step()  \n",
        "  epoch_loss.append(loss.item()) \n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epoch_loss)\n",
        "plt.title('Train Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.savefig('deep_ae_loss.png') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubZQq-LtWBBU"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        X = data[:, :6].float()\n",
        "        y = data[:,6].long()\n",
        "        output = mlp(X)\n",
        "        for idx, i in enumerate(output):\n",
        "            y_pred.append(torch.argmax(i))\n",
        "            if (torch.argmax(i) == y[idx]):\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "print(\"Accuracy: \", round(correct/total, 4))\n",
        "\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for data in train_loader:\n",
        "        X = data[:, :6].float()\n",
        "        y = data[:,6].long()\n",
        "        output = mlp(X)\n",
        "        #print(output)\n",
        "        for idx, i in enumerate(output):\n",
        "            y_pred.append(torch.argmax(i))\n",
        "confusion_matrix(train_data[:,6], y_pred)\n",
        "precision_score(train_data[:,6], y_pred)\n",
        "recall_score(train_data[:,6], y_pred)\n",
        "f1_score(train_data[:,6], y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUvlwKFXQE6F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}